import streamlit as st
import pandas as pd
import json
import asyncio
import aiohttp
import time

# è¨­å®šé é¢
st.set_page_config(page_title="Ultra Image Checker", layout="wide", page_icon="âš¡")
st.title("âš¡ æ¥µé€Ÿåœ–ç‰‡ç¶²å€æª¢æŸ¥å·¥å…· (AsyncIO ç‰ˆ)")

# --- éåŒæ­¥æª¢æŸ¥æ ¸å¿ƒé‚è¼¯ ---
async def check_url_async(session, url):
    """éåŒæ­¥æª¢æŸ¥å–®ä¸€ç¶²å€"""
    if not url or not isinstance(url, str) or not url.startswith('http'):
        return {"url": url, "status": "âš ï¸ Invalid URL", "size_kb": 0, "error": "Malformed URL"}
    
    try:
        # ä½¿ç”¨ HEAD è«‹æ±‚ï¼Œtimeout è¨­å®šç‚º 5 ç§’
        async with session.head(url, timeout=5, allow_redirects=True) as response:
            if response.status == 200:
                size_bytes = int(response.headers.get('Content-Length', 0))
                size_kb = round(size_bytes / 1024, 2)
                return {"url": url, "status": "âœ… OK", "size_kb": size_kb, "error": ""}
            else:
                return {"url": url, "status": f"âŒ Error {response.status}", "size_kb": 0, "error": f"HTTP {response.status}"}
    except Exception as e:
        return {"url": url, "status": "âš ï¸ Failed", "size_kb": 0, "error": str(e)}

async def process_batch(urls, max_concurrency, progress_bar, status_text):
    """æ§åˆ¶ä½µç™¼æ•¸é‡ä¸¦æ›´æ–°é€²åº¦"""
    connector = aiohttp.TCPConnector(limit=max_concurrency)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        for url in urls:
            task = check_url_async(session, url)
            tasks.append(task)
        
        results = []
        total = len(urls)
        
        # ä½¿ç”¨ as_completed è®“å®Œæˆçš„ä»»å‹™ç«‹å³å›å‚³ï¼Œä»¥æ›´æ–°é€²åº¦æ¢
        for i, future in enumerate(asyncio.as_completed(tasks)):
            result = await future
            results.append(result)
            
            # æ›´æ–°é€²åº¦
            percent = (i + 1) / total
            progress_bar.progress(percent)
            status_text.text(f"ğŸš€ æ­£åœ¨æª¢æŸ¥: {i + 1} / {total} ({(percent * 100):.1f}%)")
            
        return results

def extract_url(json_str):
    try:
        data = json.loads(json_str)
        return data.get('entries', {}).get('url')
    except:
        return None

# --- UI ä»‹é¢ ---
tab1, tab2 = st.tabs(["âš¡ æ‰¹é‡æ¥µé€Ÿæª¢æŸ¥", "ğŸ” å–®ä¸€ç¶²å€æª¢æŸ¥"])

# === Tab 1: æ‰¹é‡æª¢æŸ¥ ===
with tab1:
    st.header("ä¸Šå‚³ CSV é€²è¡Œå¤§é‡æƒæ")
    
    # å´é‚Šæ¬„è¨­å®š
    with st.expander("âš™ï¸ é€²éšè¨­å®š (é€Ÿåº¦æ§åˆ¶)", expanded=True):
        concurrency = st.slider(
            "åŒæ™‚ä½µç™¼é€£ç·šæ•¸ (Batch Size)", 
            min_value=10, 
            max_value=200, 
            value=50, 
            help="æ•¸å€¼è¶Šé«˜è¶Šå¿«ï¼Œä½†å¯èƒ½å°è‡´ä¼ºæœå™¨é˜»æ“‹ã€‚å»ºè­°è¨­å®š 50-100ã€‚"
        )
    
    uploaded_file = st.file_uploader("é¸æ“‡æ‚¨çš„ CSV æª”æ¡ˆ", type=["csv"], key="batch_async")

    if uploaded_file is not None:
        try:
            df = pd.read_csv(uploaded_file)
            if 'mainImage' in df.columns:
                df['extracted_url'] = df['mainImage'].apply(extract_url)
                unique_urls = df['extracted_url'].dropna().unique().tolist()
                
                st.info(f"ğŸ“Š æª”æ¡ˆè®€å–æˆåŠŸï¼æº–å‚™æª¢æŸ¥ {len(unique_urls)} å€‹ç¶²å€ã€‚")

                if st.button("ğŸš€ é–‹å§‹æ¥µé€Ÿæƒæ"):
                    # åˆå§‹åŒ– UI å…ƒä»¶
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    start_time = time.time()

                    # åŸ·è¡Œ AsyncIO
                    results = asyncio.run(process_batch(unique_urls, concurrency, progress_bar, status_text))
                    
                    end_time = time.time()
                    duration = end_time - start_time
                    
                    # é¡¯ç¤ºå®Œæˆè¨Šæ¯
                    progress_bar.progress(1.0)
                    status_text.text(f"âœ… æª¢æŸ¥å®Œæˆï¼")
                    st.success(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼è€—æ™‚: {duration:.2f} ç§’ (å¹³å‡æ¯ç§’ {len(unique_urls)/duration:.1f} å¼µ)")

                    # çµ±è¨ˆèˆ‡é¡¯ç¤º
                    results_df = pd.DataFrame(results)
                    
                    c1, c2, c3 = st.columns(3)
                    c1.metric("âœ… æ­£å¸¸", len(results_df[results_df['status'] == "âœ… OK"]))
                    c2.metric("âŒ ç•°å¸¸", len(results_df[results_df['status'] != "âœ… OK"]))
                    c3.metric("âš ï¸ å¤±æ•—", len(results_df[results_df['status'] == "âš ï¸ Failed"]))

                    st.dataframe(results_df, use_container_width=True)
                    
                    # ä¸‹è¼‰
                    csv = results_df.to_csv(index=False).encode('utf-8')
                    st.download_button(
                        "ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š",
                        data=csv,
                        file_name="async_image_report.csv",
                        mime="text/csv"
                    )
            else:
                st.error("CSV ç¼ºå°‘ 'mainImage' æ¬„ä½ï¼")
        except Exception as e:
            st.error(f"éŒ¯èª¤: {e}")

# === Tab 2: å–®ä¸€æª¢æŸ¥ (ä¿æŒä¸è®Š) ===
with tab2:
    st.header("å–®ä¸€ç¶²å€å¿«é€Ÿæ¸¬è©¦")
    url_input = st.text_input("è¼¸å…¥åœ–ç‰‡ç¶²å€")
    if st.button("æª¢æŸ¥"):
        if url_input:
            async def run_single():
                async with aiohttp.ClientSession() as session:
                    return await check_url_async(session, url_input)
            
            res = asyncio.run(run_single())
            if res['status'] == "âœ… OK":
                st.success(f"ç‹€æ…‹: {res['status']} | å¤§å°: {res['size_kb']} KB")
                st.image(url_input, width=300)
            else:
                st.error(f"ç‹€æ…‹: {res['status']} | éŒ¯èª¤: {res['error']}")
